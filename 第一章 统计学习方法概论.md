本章简要叙述统计学习方法的一些基本概念。

- 统计学习的定义、研究对象与方法
- 监督学习（本书主要内容）
- 统计学习方法的三要素：模型、策略和算法
- 模型选择，包括正则化、交叉验证与学习的泛化能力
- 生成模型与判别模型
- 监督学习方法的应用：分类问题、标注问题与回归问题

[TOC]

## 1. 统计学习

### 1.1 特点

统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，统计学习也称为统计机器学习（statistical machine learning）。

主要特点：

1. 统计学习以计算机及网络为平台，是建立在计算机及网络之上的。
2. 统计学习以数据为研究对象，是数据驱动的学科。
3. 统计学习的目的是对数据进行预测和分析。
4. 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析。
5. 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论。

赫尔伯特·西蒙（Herbert A. Simon）对"学习"给出以下定义：如果一个系统能够通过执行某个过程改进它的性能，这就是学习。

由此，可以得出：

> 统计学习就是计算机系统通过运用数据及统计方法提高系统性能的机器学习

### 1.2 统计学习的对象

统计学习的对象是数据（data），它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。

作为统计学习的对象，数据是多样的，包括存在于计算机及网络上的各种数字、文字、图像、视频、音频数据以及它们的组合。

统计学习关于数据的基本假设是**同类数据**具有一定的统计规律性，这是统计学习的前提。

>同类数据：指具有某种共同性质的数据，例如英文文章、互联网网页、数据库中的数据等。
>
>由于他们具有统计规律性，所以可以用概率统计方法来加以处理。
>
>比如，可以用随机变量描述数据中的特征，用概率分布描述数据的统计规律。

在统计学习过程中，以变量或变量组表示数据。

数据分为有连续变量和离散变量表示的类型，本书以讨论离散变量的方法为主。

### 1.3 统计学习的目的

统计学习用于对数据进行预测和分析，特别是对未知新数据进行预测与分析。

对数据的预测可以使计算机更加智能化，或者说使计算机的某些性能得到提高。

对数据的分析可以让人们获取新的知识，给人们带来新的发现。

对数据的预测与分析是通过构建概率统计模型实现的。

统计学习的总目标就是：考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提高学习效率。

### 1.4 统计学习的方法

统计学习的方法是基于数据构建统计模型从而对数据进行预测与分析。

统计学习的组成：

- 监督学习（supervised learning）
- 无监督学习（unsupervised learning）
- 半监督学习（semi-supervised learning）
- 强化学习（reinforcement learning）

本书主要讨论监督学习，这种情况下统计学习的方法可以概括如下：

- 从给定的、有限的、用于学习的训练数据（training data）集合出发，假设数据是独立同分布产生的
- 假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space）
- 应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据（test data）在给定的评价准则下有最优的预测
- 最优模型的选取由算法实现

统计学习方法的三要素：模型model（模型的假设空间）、策略strategy（模型选择的准则）、算法algorithm（模型学习的算法）

实现统计学习方法的步骤：

1. 得到一个有限的训练数据集合
2. 确定包含所有可能的模型的假设空间，即学习模型的集合
3. 确定模型选择的准则，即学习的策略
4. 实现求解最优模型的算法，即学习的算法
5. 通过学习方法选择最优模型
6. 利用学习的最优模型对新数据进行预测或分析

本书主要介绍统计学习方法为主，特别是监督学习方法，主要包括用于分类、标注与回归问题的方法。

这些方法在自然语言处理、信息检索、文本数据挖掘等领域中有着极其广泛的应用。

### 1.5 统计学习的研究

统计学习研究包括三个方面：

- 统计学习方法（statistical learning method）：开发新的学习方法
- 统计学习理论（statistical learning theory）：探索统计学习方法的有效性与效率、基本理论问题。
- 统计学习应用（application of statistical learning）：将统计学习方法应用到实际问题中去，解决实际问题。

### 1.6 统计学习的重要性

统计学习已被成功地应用到人工智能、模式识别、数据挖掘、自然语言处理、语音识别、图像识别、信息检索和生物信息等许多计算机应用领域中，并且成为这些领域的核心技术。

统计学习学科在科学技术中的重要性主要体现在以下几个方面：

1. 统计学习方法是处理海量数据的有效方法。我们处于一个信息爆炸的时代，海量数据的处理与利用是人们必然的需求。现实中的数据不但规模大，而且常常具有不确定性，统计学习往往是处理这类数据最强有力的工具。
2. 统计学习是计算机智能化的有效手段。智能化是计算机发展的必然趋势，也是计算机技术研究与开发的主要目标。近几十年来，人工智能等领域的研究表明，利用统计学习模仿人类智能的方法，虽有一定的局限性，但仍然是实现这一目标的最有效手段。
3. 统计学习是计算机科学发展的一个重要组成部分。可以认为计算机科学由三维组成：系统、计算、信息。统计学习主要属于信息这一维，并在其中起着核心作用。

## 2. 监督学习

 监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测(这里的输入、输出是指某个系统的输入与输出，不是学习的输入和输出)。

计算机的基本操作就是给定一个输入产生一个输出，所以监督学习是极其重要的统计学习分支，也是统计学习中内容最丰富、应用最广泛的部分。

### 2.1 基本概念

#### 2.1.1 输入空间、特征空间与输出空间

在监督学习中，将输入与输出所有可能取值的集合分别称为**输入空间(input space)**与**输出空间(output space)**。

输入与输出空间可以是有限元素的集合，也可以是整个欧式空间。

输入空间与输出空间可以是同一个空间，也可以是不同的空间，但通常输入空间远远大于输出空间。

每个具体的输入是一个实例(instance)，通常由特征向量(feature vector)表示。这时，所有特征向量存在的空间称为特征空间(feature space)。特征空间的每一维对应一个特征。

- **有时假设输入空间与特征空间为相同的空间，对它们不予区分。**
- **有时假设输入空间与特征空间为不同的空间，将实例从输入空间映射到特征空间。**

模型实际上都是定义在特征空间上的。

在监督学习过程中，将输入与输出看作是定义在输入(特征)空间与输出空间上的随机变量的取值。

- 输入输出变量用大写字母表示，习惯上输入变量写作$X$，输出变量写作$Y$
- 输入输出变量所取的值用小写字母表示，输入变量的取值写作$x$，输出变量的取值写作$y$
- 变量可以是标量或向量，都用相同类型字母表示

输入实例$x$的特征向量记做

$$
x=(x^{(1)}, x^{(2)},\dots, x^{(i)},\dots,x^{(n)})^{T}\tag{2.1}
$$
其中，x^{(i)}表示x的第i个特征。

**注意：$ x^{(i)}$与$x_{i}$不同，本书通常用$x_{i}$表示多个输入变量中的第$i$个，即**$x_{i}=(x^{(1)}_{i}, x^{(2)}_{i},\dots, x^{(i)}_{i},\dots,x^{(n)}_{i})^{T}$

监督学习从训练数据(training data)集合中学习模型，对测试数据(test data)进行预测。训练数据由输入(或特征向量)与输出对组成，训练集通常表示为：$T=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{N},y_{N})\}$

测试数据也由相应的输入与输出对组成。输入与输出对又称为样本(sample)或样本点。

输入变量$X$和输出变量$Y$有不同的类型，可以是连续的，也可以是离散的。

人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：

- 输入变量与输出变量均为连续变量的预测问题称为**回归问题**
- 输入变量为有限个离散变量的预测问题称为**分类问题**
- 输入变量与输出变量均为变量序列的预测问题称为**标注问题**

#### 2.1.2 联合概率分布

监督学习假设输入与输出的随机变量$X$和$Y$遵循联合概率分布$P(X,Y)$，$P(X,Y)$表示分布函数，或分布密度函数。

**注意：在学习过程中，假定这一联合概率分布存在，但对学习系统来说，联合概率分布的具体定义是未知的**

训练数据与测试数据被看作是依联合概率分布$P(X,Y)$独立同分布产生的。统计学习假设数据存在一定的统计规律，$X$和$Y$具有联合概率分布的假设就是监督学习关于数据的基本假设。

#### 2.1.3 假设空间

监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。学习的目的就在于找到最好的这样的模型。

模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间(hypothesis space)。假设空间的确定意味着学习范围的确定。

监督学习的模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数(decision function)$Y=f(X)$表示，随具体学习方法而定。对具体的输入进行相应的输出预测时，写作$P(y|x)$或$y=f(x)$。

### 2.2 问题的形式化

监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测(prediction)。

由于在这个过程中需要训练数据集，而训练数据集往往是人工给出的，所以称为监督学习。

监督学习分为**学习**和**预测**两个过程，由学习系统与预测系统完成，如图1.1描述。

<img src="./images/图1.1 监督学习问题.png" width=50%>

首先给定一个训练数据集：$T=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{N},y_{N})\}$

其中$(x_{i},y_{i})$，$i=1,2,\dots,N$，称为样本或样本点。$x_{i}\in{X}\subseteq\textbf{R}^{n}$是输入的观测值，也称为输入或实例，$y_{i}\in{Y}$是输出的观测值，也称为输出。

监督学习中，假设训练数据与测试数据是依联合概率分布$P(X,Y)$独立同分布产生的。

在学习过程中，学习系统利用给定的训练数据集，通过学习(或训练)得到一个模型，表示为条件概率分布$\hat{P}(Y|X)$或决策函数$Y=\hat{f}(X)$。条件概率分布$\hat{P}(Y|X)$或决策函数$Y=\hat{f}(X)$描述输入与输出随机变量之间的映射关系。

在预测过程中，预测系统对于给定的测试样本集中的输入$x_{N+1}$，

由模型$y_{N+1}=\arg \max \limits_{y_{N+1}} \hat{P}(y_{N+1}|x_{N+1})$或$y_{N+1}=\hat{f}(x_{N+1})$给出相应的输出$y_{n+1}$

在学习过程中，学习系统(即学习算法)试图通过训练数据集中的样本$(x_{i},y_{i})$带来的信息学习模型。

具体地说，对输入$x_{i}$，一个具体的模型$y=f(x)$可以产生一个输出$f(x_{i})$，而训练数据集中对应的输出是$y_{i}$，如果这个模型有很好的预测能力，训练样本输出$y_i$和模型输出$f(x_{i})$之间的差就应该足够小。

学习系统通过不断的尝试，选取最好的模型，以便对训练数据集有足够好的预测，同时对未知的测试数据集的预测也有尽可能好的推广。

## 3. 统计学习三要素

统计学习方法都是由**模型**、**策略**和**算法**构成的，即统计学习方法由三要素构成，可以简单地表示为：

**方法 = 模型 + 策略 + 算法**

可以说构建一种统计学习方法就是确定具体的统计学习三要素

### 3.1 模型

统计学习首要考虑的问题是学习什么样的模型。

在监督学习过程中，**模型就是所要学习的条件概率分布或决策函数。**

模型的假设空间(hypithesis space)包括所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。**假设空间中的模型一般有无穷多个。**

假设空间用$\digamma$表示，假设空间可以定义为决策函数的集合：
$$
\digamma=\{f|Y=f(X)\}\tag{3.1}
$$
其中，$X$和$Y$是定义在输入空间和输出空间上的变量。这时$\digamma$通常是由一个参数向量决定的函数族：
$$
\digamma=\{f|Y=f_{\theta}(X),\theta\in{\textbf{R}^{n}}\}\tag{3.2}
$$
参数向量$\theta$取值于$n$维欧式空间$\textbf{R}^{n}$，称为参数空间(parameter space)。

假设空间也可以定义为条件概率的集合：
$$
\digamma=\{P|P(Y|X)\}\tag{3.3}
$$
其中，$X$和$Y$是定义在输入空间和输出空间上的随机变量。这时$\digamma$通常是由一个参数向量决定的条件概率分布族：
$$
\digamma=\{P|P_{\theta}(Y|X),\theta\in{\textbf{R}^n}\}\tag{3.4}
$$
参数向量$\theta$取值于$n$为欧式空间$\textbf{R}^{n}$，也称为参数空间(parameter space)。

本书中称由**决策函数**表示的模型为非概率模型，由条件概率表示的模型为概率模型，为了简便起见，当论及模型时，有时只用其中一种模型。

### 3.2 策略

有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。

**统计学习的目标在于从假设空间中选取最优模型。**

首先引入**损失函数**与**风险函数**的概念。

- 损失函数：度量模型一次预测的好坏
- 风险函数：度量平均意义下模型预测的好坏

#### 3.2.1 损失函数和风险函数

**监督学习问题是在假设空间$\digamma$中选取模型$f$作为决策函数，对于给定的输入$X$，由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。**

损失函数是$f(X)$和$Y$的非负实值函数，记作$L(Y,f(X))$。

统计学习常用的损失函数有以下几种：

1. 0-1损失函数(0-1 loss function)

$$
L(Y,f(X))=\left\{
\begin{aligned}
1,\quad Y \neq f(X)\\
0,\quad	Y = f(X) \\
\end{aligned}
\right.\tag{3.5}
$$

2. 平方损失函数(quadratic loss function)

$$
L(Y,f(X))=(Y-f(X))^{2}\tag{3.6}
$$

3. 绝对损失函数(absolute loss function)

$$
L(Y,f(X))=|Y-f(X)|\tag{3.7}
$$

4. 对数损失函数(logarithmic loss function)或对数似然损失函数(loglikelihood loss function)

$$
L(Y,P(Y|X))=-logP(Y|X)\tag{3.8}
$$

**损失函数值越小，模型就越好。**

由于模型的输入、输出$(X,Y)$是随机变量，遵循联合分布$P(X,Y)$，所以损失函数的期望是：
$$
R_{exp}(f)=E_{p}[L(Y,f(X))]=\int_{X\times{Y}}L(y,f(x))P(x,y)dxdy\tag{3.9}
$$
这是理论上的模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数(risk funcion)或期望损失(expected loss)。**学习的目标就是选择期望风险最小的模型。**

**由于联合分布$P(X,Y)$是未知的，$R_{exp}(f)$不能直接计算。**

实际上，如果知道联合分布$P(X,Y)$，可以从联合分布直接求出条件概率分布$P(Y|X)$，也就不需要学习了。正因为不知道联合概率分布，所以才需要进行学习。

这样一来，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就成为一个病态问题(ill-formed problem)。

给定一个训练数据集：
$$
T=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{N},y_{N})\}\tag{3.10}
$$
模型$f(X)$关于训练数据集的平均损失称为经验风险(empirical risk)或经验损失(empirical loss)，记作$R_{emp}$：
$$
R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_{i},f(x_{i}))\tag{3.11}
$$

- **期望风险$R_{exp}(f)$是模型关于联合分布的期望损失**
- **经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失**

根据大数定律，当样本容量$N$趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$。所以一个很自然的想法是用经验风险估计期望风险。

但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：**经验风险最小化**和**结构风险最小化**。

#### 3.2.2 经验风险最小化与结构风险最小化

在假设空间、损失函数以及训练数据集确定的却倾下







